### YamlMime:FAQ
metadata:
  title: Frequently asked questions about exporting Microsoft Dataverse table data to Azure Synapse Analytics and Azure Data Lake
  description: Get answers to frequently asked questions about Power Apps Azure Synapse Link for Dataverse.
  author: sabinn-msft
  ms.search.keywords: 
  ms.date: 12/13/2023
  ms.author: sabinn
  ms.reviewer: 
  contributors: JasonHQX
  ms.topic: faq
title: Azure Synapse Link for Dataverse FAQ
summary: This article provides information on frequently asked questions about exporting Microsoft Dataverse table data to Azure Synapse Analytics and Azure Data Lake.
sections:
- name: General
  questions:
   - question: Can I manually perform tasks such as creating, updating, deleting, or setting retention policies for data files in the connected Azure storage?
     answer: |
        Data files shouldn't be modified by a customer and no customer files should be placed in the data folders.
        > [!NOTE]
        > To drop stale and stagnant data in the data lake without breaking the Azure Synapse Link, you should consider using the feature [Query and analyze the incremental updates](azure-synapse-link-incremental.md)
   - question: Can I link a Synapse workspace to an existing Azure Synapse Link profile with data lake only?
     answer: |
        Yes. In your web browser's address bar, append `?athena.updateLake=true` to the web address that ends with **exporttodatalake**. When you select an existing profile from the Azure Synapse Link homepage, you'll see a new action in the extended option **Link to Azure Synapse Analytics Workspace**.  
   - question: How can I access my table relationships?
     answer: |
        To access many-to-many relationships, the relationship will be available as a table to select from the **Add tables** page for a new link and from the **Manage tables** for a pre-existing link.
   - question: How can I get estimated costs before adding Azure Synapse Link?
     answer: |
        Azure Synapse Link is a free OOB solution in Dataverse. Utilizing Azure Synapse Link for Dataverse does not incur additional charges under Dataverse. However, consider potential costs for Azure service:
        - Data storage in ADLS gen2:  [Azure Storage Data Lake Gen2 Pricing | Microsoft Azure](https://azure.microsoft.com/en-us/pricing/details/storage/data-lake/)
        - Data consumption cost (e.g., Synapse Workspace):  [Pricing - Azure Synapse Analytics | Microsoft Azure](https://azure.microsoft.com/en-us/pricing/details/synapse-analytics/)
        For comprehensive Azure cost management information, please check: [Plan to manage Azure costs - Microsoft Cost Management | Microsoft Learn](https://learn.microsoft.com/en-us/azure/cost-management-billing/understand/plan-manage-costs)
   - question: What happens when I add a column?
     answer: |
        When you add a new column to a table in the source, it's also added at the end of the file in the destination in the corresponding file partition. While the rows that existed prior to the addition of the column won't show the new column, new or updated rows will show the newly added column.
   - question: What happens when I delete a column?
     answer: |
        When you delete a column from a table in the source, the column isn't dropped from the destination. Instead, the rows are no longer updated and are marked as null while preserving the previous rows.
   - question: What happens if I change the data type of a column?
     answer: |
        Changing the data type of a column is a breaking change and you'll need to unlink and relink.
   - question: What happens when I delete a row?
     answer: |
        Deleting a row is handled differently based on which data write options you choose:
        - In-place update: This is the default mode and when you delete a table row in this mode, the row is also deleted from the corresponding data partition in the Azure Data Lake. In other words, data is hard deleted from the destination. 
        - Append-only: In this mode, when a Dataverse table row is deleted, it isn't hard deleted from the destination. Instead, a row is added and set as isDeleted=True to the file in the corresponding data partition in Azure Data Lake.
   - question: Why I am not seeing column header in exported file?
     answer: |
        Azure Synapse Link follow Common Data Model to makes it possible for data and its meaning to be shared across applications and business processes such as Microsoft PowerApps, Power BI, Dynamics 365, and Azure. In each CDM folder, metadata like column header is stored in model.json file. For more information: [Common Data Model and Azure Data Lake Storage Gen2 - Common Data Model | Microsoft Learn](https://learn.microsoft.com/en-us/common-data-model/data-lake)   
   - question:  Why does the Model.json file increase or change in length for the data types and doesn't keep what is defined in Dataverse?
     answer: |
        Model.json keeps the database length for the column's size. Dataverse has a concept of database length for each column. If you create a column with a size of 200 and later reduce it to 100, Dataverse still allows your existing data to be present in Dataverse. It does that by keeping `DBLength` to 200 and MaxLength to 100. What you see in Model.json is `DBLength`` and if you use that for downstream processes you'll never provision lesser space for your Dataverse columns.
   - question: What date and time formats can be expected in exported Dataverse tables?
     answer: |
        There are three date and time formats that can be expected in the exported Dataverse tables.
          
        | Column Name                              | Format                           | Data Type                          | Example                            |
        |------------------------------------------|----------------------------------|------------------------------------|------------------------------------|
        | **SinkCreatedOn** and **SinkModifiedOn** | M/d/yyyy H:mm:ss tt              | datetime                           | 6/28/2021 4:34:35 PM               |
        | **CreatedOn**                            | yyyy-MM-dd'T'HH:mm:ss.sssssssXXX | datetimeOffset                     | 2018-05-25T16:21:09.0000000+00:00  |
        | All Other Columns                        | yyyy-MM-dd'T'HH:mm:ss'Z'         | datetime                           | 2021-06-25T16:21:12Z               |
        
        > [!NOTE]
        > **CreatedOn** data type changed from datetime to datetimeOffset on 07/29/2022. To edit the data type format for a table created before the change, drop and readd the table.
        > 
        > You can choose different column behaviors for a Date and Time column in Dataverse, which updates the data type format. More information: [Behavior and format of the Date and Time column](behavior-format-date-time-field.md)
   - question: When should I use a yearly or monthly partition strategy?
     answer: |
        For Dataverse tables where data volume is high within a year, we recommend you use monthly partitions. Doing so results in smaller files and better performance. Additionally, if the rows in Dataverse tables are updated frequently, splitting into multiple smaller files helps improve performance in the case of in-place update scenarios.
   - question: What is append only mode and what is the difference between append only and in-place update mode?
     answer: |
        In append only mode, incremental data from Dataverse tables are appended to the corresponding file partition in the lake. For more information: [Advanced Configuration Options in Azure Synapse Link](azure-synapse-link-advanced-configuration.md)
   - question: When do I use append only mode for a historical view of changes?
     answer: |
         Append only mode is the recommended option for writing Dataverse table data to the lake, especially when the data volumes are high within a partition with frequently changing data. Again, this is a commonly used and highly recommended option for enterprise customers. Additionally, you can choose to use this mode for scenarios where the intent is to incrementally review changes from Dataverse and process the changes for ETL, AI, and ML scenarios. Append only mode provides a history of changes, instead of the latest change or in place update, and enables several time series from AI scenarios, such as prediction or forecasting analytics based on historical values. 
   - question: How do I retrieve the most up-to-date row of each record and exclude deleted rows when I export data in append only mode?
     answer: |
        In append only mode, you should identify the latest version of record with the same ID using **VersionNumber** and **SinkModifiedOn** then apply **isDeleted=0** on the latest version.
   - question: If I drop and relink the table, will VersionNumber make any changes?
     answer: |
        **VersionNumber** is a rowversion data type and will be changed after any operation.
   - question: Why do I see duplicated version numbers when I export data using append only mode?   
     answer: |
         For append only mode, if Azure Synapse Link for Dataverse doesn't get an acknowledgment from the Azure data lake that the data has been committed due to any reason such as network delays, Azure Synapse Link will retry in those scenarios and commit the data again. The downstream consumption should be made resilient to this scenario by filtering data using SinkModifiedOn.     
   - question: Why am I seeing differences in Sinkmodifiedon and Modifiedon columns?   
     answer: |
         It is expected: Modifiedon is the datetime that record being changed in Dataverse; Sinkmodifiedon is the datetime that record being modified in the Datalake.  
   - question: Which Dataverse tables aren't supported for export?
     answer: |
        Any table that doesn't have change tracking enabled won't be supported in addition to following system tables:
        - Attachment
        - Calendar
        - Calendarrule

        > [!NOTE]
        > You can add the audit table for export using Azure Synapse Link for Dataverse. However, the export of the audit table is only supported with [Delta Lake profiles](/power-apps/maker/data-platform/azure-synapse-link-delta-lake).
   - question: I am using export to delta lake feature, can I stop the Apache Spark job or change the execution time?
     answer: |
        The Delta Lake conversion job will be triggered if there was a data change in the configured time interval. There is no option to stop/pause the Apache Spark Pool. However, user can modify the time interval after the link creation under Manage tables – Advanced – Time interval.
   - question: Does Azure Synapse Link support calculated columns?
     answer: |
        Calculated columns are only supported when the lookup field is located within the same table. Data updates occur only when change tracking is triggered: Lookup values will change on the root tables only when the root table records are changed. To better reflect the value of a lookup field, it's recommended to join with the original table to get the latest value.
   - question: Which Dataverse tables use append only mode by default?
     answer: |
        All tables that don't have a createdOn field will be synced using append only mode by default. This includes relationship tables as well as the ActivityParty table.
   - question: Why does Azure Synapse Link for Dataverse require all resources to be in the same region and what can I do about it?
     answer: |
        To ensure high performance and low latency in addition to preventing egress charges, Azure Synapse Link requires all resources to be located in the same region. If you have a cross-region scenario, you can:
        - Move the Azure resources to the same region as the environment.
        - Move the environment to the same region as the Azure resources by contacting [Microsoft customer support](/power-platform/admin/get-help-support).
        - Enable **Read Access - Geo Redundant Storage (RA-GRS)** on the Azure Data Lake to replicate Azure data to a near by region. More information: [Read access to data in the secondary region](/azure/storage/common/storage-redundancy#read-access-to-data-in-the-secondary-region)
        - Use Azure Synapse pipelines or Azure Data Factory to copy data from Azure resources in one region to Azure resources in another.
   - question: Why do I see the error message - Content of directory on path cannot be listed?
     answer: |
        Dataverse data is stored in the connected storage container. You need the "Storage Blob Data Contributor" role in the linked storage account to perform read and query operations through Synapse Workspace.
   - question: Why do I see the error message - can't bulk load because the file is incomplete or couldn't be read?
     answer: |
        Dataverse data can continuously change through creating, updating, and deleting transactions. This error is caused by the underlying file being changed when you're reading data from it. So, for tables with continuous changes, you should change your consumption pipeline to use snapshot data (partitioned tables) to consume. More information: [Create an Azure Synapse Link for Dataverse with your Azure Synapse Workspace](azure-synapse-link-synapse.md#access-near-real-time-data-and-read-only-snapshot-data)
   - question: How can I use Azure Synapse Link to archive critical data?
     answer: |
        Synapse Link for DV is designed for analytics purposes, we recommend customers to use long-term-retention for archive purposes: [Dataverse long term data retention overview - Power Apps | Microsoft Learn](https://learn.microsoft.com/en-us/power-apps/maker/data-platform/data-retention-overview)

additionalContent: |

  ## See also

  [Azure Synapse Link for Dataverse](export-to-data-lake.md)
